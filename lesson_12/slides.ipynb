{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices and recommendations\n",
    "\n",
    "### Регрессия. Использование классификационных моделей для регрессии.\n",
    "    * взвешенный KNN\n",
    "![](https://i.stack.imgur.com/gAILq.png)\n",
    "    * Decision Tree\n",
    "![](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/_images/plot_tree_regression_1.png)\n",
    "    * Boosting\n",
    "![](http://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_regression_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices\n",
    "1. Будет сложнее чем кажется.\n",
    "2. Обещай меньше, делай больше.\n",
    "3. Принцип Парето - 80% результата достигается за 20% времени. Имейте разумные ожидания и метрики когда нужно остановиться.\n",
    "4. Знай свои данные\n",
    "    * Что такое нули в данных?\n",
    "    * Что означают отсутствующие данные?\n",
    "    * Что означают статистические выбросы? Игнорировать или фокусироваться?\n",
    "    * Откуда пришли данные для обучения, кто присваивал классы?\n",
    "    * Имеет ли значение порядок строк в датасете?\n",
    "5. Документировать всё!\n",
    "\n",
    "![](http://scikit-learn.org/stable/_static/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм построения модели\n",
    "\n",
    "0. Выберите метрику для своей задачи!\n",
    "1. Разделите данные на обучающую выборку и тестовую. \n",
    "2. Используя обучающую выборку - найдите оптимальные параметры с помощью кроссвалидации.\n",
    "3. Используя оптимальные параметры - обучите модель на всей обучающей выборке.\n",
    "4. Проверьте полученную модель используя выбранную метрику на тестовых данных - это итоговый результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация\n",
    "1. Будьте осторожны - не допускайте \"протечек\" информации из тестовой выборки.\n",
    "2. Думайте о том, когда она полезна.\n",
    "3. Нормализация после PCA может быть вредной. Нормализация ДО - необходима.\n",
    "4. Экспериментируйте :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Несбалансированные данные\n",
    "\n",
    "1. Subsample / oversample\n",
    "2. Использование весов.\n",
    "3. Stratified CV\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отсутствующие данные\n",
    "\n",
    "1. Выкинуть? - может привести к слишком маленькому датасету\n",
    "2. Заполнить средним? - не зависит от других значений в записи\n",
    "3. Использовать регрессию для заполнения? - Затратно, не всегда возможно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекоммендационные системы\n",
    "\n",
    "Источники:\n",
    "1. [\"Collaborative Filtering Recommender Systems\"](http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf)\n",
    "2. [\"Collaborative Filtering\" - Stanford slides](http://web.stanford.edu/~lmackey/papers/cf_slides-pml09.pdf)\n",
    "\n",
    "Два с половиной подхода:\n",
    "1. [Коллаборативная фильтрация](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BB%D0%BB%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D1%84%D0%B8%D0%BB%D1%8C%D1%82%D1%80%D0%B0%D1%86%D0%B8%D1%8F). Основная идея - людям, похожим на меня, нравятся вещи как мне. Люди, которым нравятся вещи как мне - похожи на меня. Абсолютно не зависит от данных пользователя. Работает, как по пользователям, так и по рекомендованным элементам. Требует некоторого времени/данных для настройки.\n",
    "2. [Фильтрация на основании данных контента](http://recommender-systems.org/content-based-filtering/). Каждый элемент описывается параметрами. На основании параметров вычисляются \"расстояния\" между элементами. Рекомендуются элементы близкие к понравившимся пользователю (и далекие от непонравившихся). Работает даже для 1й записи.\n",
    "3. В качестве простейшей рекоммендационной системы можно расценивать оценку как отсутствующие данные и действовать так же как в общем случае. \n",
    "    * Заполнить средним\n",
    "    * Свести к регрессии. Чем хорош KNN? Что такое расстояние в данном контексте? Коэффициент Пирсона, [cosine similarity](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%9E%D1%82%D0%B8%D0%B0%D0%B8)\n",
    "    * [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "\n",
    "## [MapReduce](https://en.wikipedia.org/wiki/MapReduce)\n",
    "\n",
    "Что такое MapReduce?\n",
    "\n",
    "* Модель программирования для больших наборов данных.\n",
    "* Параллельный распределенный алгоритм\n",
    "* Фреймворк для кластера\n",
    "* Способ мышления\n",
    "\n",
    "Изначально разработан гуглом.  \n",
    "[Hadoop](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html) - Популярная имплементация MapReduce на Java  \n",
    "MrJob - Python интерфейс\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTwjmNVU_9E_2AN4zC_pB7M8uT38Tuf71XlZRAl7gv7Oemzo928)\n",
    "\n",
    "2 главных шага.\n",
    "\n",
    "1. MAP - фильтрация и сортировка\n",
    "2. REDUCE - аггрегация результатов\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcounter1.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcounter1.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class mrWordCount(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mrWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/wordcounter1.modintsov.20170228.085828.322637\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Streaming final output from /tmp/wordcounter1.modintsov.20170228.085828.322637/output...\n",
      "\"\"\t4\n",
      "\"a\"\t3\n",
      "\"ac\"\t4\n",
      "\"adipiscing\"\t1\n",
      "\"aenean\"\t1\n",
      "\"aliquam\"\t2\n",
      "\"aliquam.\"\t1\n",
      "\"aliquet\"\t2\n",
      "\"amet\"\t2\n",
      "\"amet,\"\t1\n",
      "\"at\"\t6\n",
      "\"auctor\"\t4\n",
      "\"auctor,\"\t1\n",
      "\"bibendum\"\t3\n",
      "\"bibendum,\"\t1\n",
      "\"blandit\"\t1\n",
      "\"condimentum\"\t1\n",
      "\"congue\"\t4\n",
      "\"consectetur\"\t3\n",
      "\"consequat\"\t1\n",
      "\"convallis\"\t1\n",
      "\"cras\"\t2\n",
      "\"curabitur\"\t2\n",
      "\"cursus\"\t2\n",
      "\"dapibus\"\t1\n",
      "\"diam\"\t1\n",
      "\"diam.\"\t1\n",
      "\"dictum\"\t1\n",
      "\"dictumst.\"\t1\n",
      "\"dignissim\"\t5\n",
      "\"dolor\"\t2\n",
      "\"dolor.\"\t1\n",
      "\"donec\"\t4\n",
      "\"dui\"\t1\n",
      "\"duis\"\t3\n",
      "\"efficitur\"\t3\n",
      "\"egestas\"\t1\n",
      "\"eget\"\t6\n",
      "\"eget,\"\t2\n",
      "\"eget.\"\t2\n",
      "\"eleifend.\"\t1\n",
      "\"elementum\"\t1\n",
      "\"elementum.\"\t1\n",
      "\"elit\"\t1\n",
      "\"elit,\"\t1\n",
      "\"elit.\"\t1\n",
      "\"enim\"\t2\n",
      "\"enim.\"\t3\n",
      "\"erat\"\t3\n",
      "\"erat.\"\t1\n",
      "\"eros\"\t1\n",
      "\"eros,\"\t1\n",
      "\"est\"\t2\n",
      "\"est,\"\t1\n",
      "\"est.\"\t1\n",
      "\"et\"\t4\n",
      "\"et,\"\t1\n",
      "\"et.\"\t1\n",
      "\"etiam\"\t1\n",
      "\"eu\"\t3\n",
      "\"eu,\"\t1\n",
      "\"euismod\"\t2\n",
      "\"ex\"\t2\n",
      "\"ex.\"\t2\n",
      "\"facilisis\"\t1\n",
      "\"faucibus\"\t3\n",
      "\"felis\"\t2\n",
      "\"felis.\"\t1\n",
      "\"fermentum\"\t2\n",
      "\"finibus\"\t1\n",
      "\"finibus,\"\t1\n",
      "\"fringilla\"\t2\n",
      "\"fusce\"\t2\n",
      "\"gravida\"\t2\n",
      "\"habitasse\"\t1\n",
      "\"hac\"\t1\n",
      "\"hendrerit\"\t2\n",
      "\"iaculis\"\t1\n",
      "\"iaculis.\"\t1\n",
      "\"id\"\t7\n",
      "\"imperdiet\"\t2\n",
      "\"in\"\t6\n",
      "\"in,\"\t1\n",
      "\"integer\"\t2\n",
      "\"interdum.\"\t1\n",
      "\"ipsum\"\t3\n",
      "\"ipsum.\"\t1\n",
      "\"justo\"\t2\n",
      "\"justo.\"\t2\n",
      "\"lacinia\"\t3\n",
      "\"lacus,\"\t1\n",
      "\"laoreet\"\t1\n",
      "\"laoreet.\"\t1\n",
      "\"lectus\"\t3\n",
      "\"lectus,\"\t1\n",
      "\"leo\"\t1\n",
      "\"libero,\"\t1\n",
      "\"libero.\"\t1\n",
      "\"ligula\"\t2\n",
      "\"ligula,\"\t1\n",
      "\"ligula.\"\t1\n",
      "\"lobortis\"\t1\n",
      "\"lobortis.\"\t1\n",
      "\"lorem\"\t1\n",
      "\"luctus\"\t2\n",
      "\"maecenas\"\t2\n",
      "\"magna\"\t4\n",
      "\"massa\"\t5\n",
      "\"massa,\"\t1\n",
      "\"mauris\"\t2\n",
      "\"mauris.\"\t2\n",
      "\"maximus\"\t1\n",
      "\"metus.\"\t1\n",
      "\"mi\"\t1\n",
      "\"molestie\"\t1\n",
      "\"molestie.\"\t1\n",
      "\"mollis,\"\t1\n",
      "\"mollis.\"\t1\n",
      "\"nam\"\t3\n",
      "\"nec\"\t4\n",
      "\"nec,\"\t1\n",
      "\"nec.\"\t1\n",
      "\"neque\"\t1\n",
      "\"neque.\"\t1\n",
      "\"nibh\"\t1\n",
      "\"nibh,\"\t1\n",
      "\"nisi\"\t3\n",
      "\"nisl\"\t2\n",
      "\"nisl,\"\t2\n",
      "\"nisl.\"\t1\n",
      "\"non\"\t6\n",
      "\"nulla\"\t5\n",
      "\"nullam\"\t1\n",
      "\"nunc\"\t3\n",
      "\"nunc,\"\t2\n",
      "\"nunc.\"\t1\n",
      "\"odio,\"\t2\n",
      "\"orci\"\t3\n",
      "\"pellentesque\"\t4\n",
      "\"pharetra\"\t3\n",
      "\"phasellus\"\t3\n",
      "\"placerat\"\t3\n",
      "\"platea\"\t1\n",
      "\"porta\"\t1\n",
      "\"porta,\"\t1\n",
      "\"porttitor\"\t2\n",
      "\"porttitor.\"\t3\n",
      "\"posuere\"\t1\n",
      "\"praesent\"\t2\n",
      "\"proin\"\t1\n",
      "\"pulvinar\"\t2\n",
      "\"purus\"\t1\n",
      "\"purus.\"\t3\n",
      "\"quam\"\t3\n",
      "\"quam.\"\t1\n",
      "\"quis\"\t6\n",
      "\"quis,\"\t1\n",
      "\"quisque\"\t2\n",
      "\"rhoncus\"\t1\n",
      "\"risus\"\t1\n",
      "\"risus,\"\t1\n",
      "\"rutrum\"\t1\n",
      "\"rutrum.\"\t1\n",
      "\"sapien.\"\t1\n",
      "\"scelerisque\"\t1\n",
      "\"scelerisque,\"\t1\n",
      "\"sed\"\t5\n",
      "\"sed,\"\t1\n",
      "\"sem\"\t3\n",
      "\"sem,\"\t2\n",
      "\"sem.\"\t1\n",
      "\"sit\"\t3\n",
      "\"sodales\"\t1\n",
      "\"sodales.\"\t1\n",
      "\"sollicitudin\"\t1\n",
      "\"suspendisse\"\t2\n",
      "\"tempor\"\t4\n",
      "\"tempor.\"\t1\n",
      "\"tempus\"\t3\n",
      "\"tincidunt\"\t5\n",
      "\"tincidunt.\"\t2\n",
      "\"tortor\"\t1\n",
      "\"tortor.\"\t1\n",
      "\"tristique\"\t2\n",
      "\"tristique.\"\t1\n",
      "\"turpis\"\t4\n",
      "\"turpis,\"\t1\n",
      "\"ullamcorper\"\t2\n",
      "\"ultrices\"\t1\n",
      "\"ultrices.\"\t1\n",
      "\"ultricies\"\t1\n",
      "\"urna\"\t2\n",
      "\"ut\"\t3\n",
      "\"ut,\"\t1\n",
      "\"varius\"\t1\n",
      "\"vehicula\"\t1\n",
      "\"vel\"\t6\n",
      "\"vel,\"\t2\n",
      "\"venenatis\"\t1\n",
      "\"vestibulum\"\t5\n",
      "\"vitae\"\t6\n",
      "\"vivamus\"\t1\n",
      "\"viverra\"\t3\n",
      "\"volutpat\"\t3\n",
      "\"volutpat,\"\t2\n",
      "\"volutpat.\"\t1\n",
      "Removing temp directory /tmp/wordcounter1.modintsov.20170228.085828.322637...\n"
     ]
    }
   ],
   "source": [
    "!/home/modintsov/.virtualenvs/ds2017/bin/python wordcounter1.py < loremipsum.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://1.bp.blogspot.com/-Nm8n33ZWE5o/ToCbSHYVwfI/AAAAAAAAQ2Q/9_I_1l0QpW8/s1600/education-trends.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing anagram.py\n"
     ]
    }
   ],
   "source": [
    "%%file anagram.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRAnagram(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Convert word into a list of characters, sort them, and convert\n",
    "        # back to a string.\n",
    "        letters = list(line)\n",
    "        letters.sort()\n",
    "\n",
    "        # Key is the sorted word, value is the regular word.\n",
    "        yield letters, line\n",
    "\n",
    "    def reducer(self, _, words):\n",
    "        # Get the list of words containing these letters.\n",
    "        anagrams = [w for w in words]\n",
    "\n",
    "        # Only yield results if there are at least two words which are\n",
    "        # anagrams of each other.\n",
    "        if len(anagrams) > 1:\n",
    "            yield len(anagrams), anagrams\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRAnagram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/anagram.modintsov.20170228.093500.829051\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Streaming final output from /tmp/anagram.modintsov.20170228.093500.829051/output...\n",
      "Removing temp directory /tmp/anagram.modintsov.20170228.093500.829051...\n"
     ]
    }
   ],
   "source": [
    "!/home/modintsov/.virtualenvs/ds2017/bin/python anagram.py < words.txt > anagrams.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t[\"st\", \"ts\"]\r\n",
      "2\t[\"usw\", \"wus\"]\r\n",
      "2\t[\"su\", \"us\"]\r\n",
      "2\t[\"sv\", \"vs\"]\r\n",
      "2\t[\"sw\", \"ws\"]\r\n",
      "2\t[\"tty\", \"tyt\"]\r\n",
      "2\t[\"tu\", \"ut\"]\r\n",
      "2\t[\"tv\", \"vt\"]\r\n",
      "2\t[\"ux\", \"xu\"]\r\n",
      "2\t[\"\", \"\"]\r\n"
     ]
    }
   ],
   "source": [
    "!tail anagrams.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важность локальной аггрегации.\n",
    "\n",
    "Идеальные характеристики масштабирования:\n",
    "\n",
    "* В два раза больше данных - в два раза дольше вычисления\n",
    "* В два раза больше ресурсов - в два раза короче вычисления\n",
    "\n",
    "Почему это недостижимый идеал?\n",
    "\n",
    "* Синхронизация требует коммуникации\n",
    "* Коммуникация убивает производительность\n",
    "\n",
    "Значит надо найти способ избежать коммуникаций!\n",
    "\n",
    "* Уменьшаем количество промежуточных данных локальной аггрегацией.\n",
    "* 2 варианта: combiners, in-mapper combining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Combiner](https://www.tutorialspoint.com/map_reduce/map_reduce_combiners.htm)\n",
    "\n",
    "![](https://image.slidesharecdn.com/hadoopmr-150613152227-lva1-app6891/95/xml-parsing-with-map-reduce-35-638.jpg?cb=1434209024)\n",
    "\n",
    "* Мини-reduce\n",
    "* Получает данные только из своего маппера\n",
    "* Значительно уменьшает сетевой траффик\n",
    "* Не имеет доступа к другим мапперам\n",
    "* Не гарантирует что имеет все данные по ключу\n",
    "* Не гарантирует что будет запущен вовсе!\n",
    "* Может быть запущен несколько раз!\n",
    "* Формат результата должен соответствовать формату результата маппера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcounter2.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcounter2.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class mrWordCount(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "            yield word.lower(), 1\n",
    "            \n",
    "    def combiner(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "    def reducer(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mrWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/wordcounter2.modintsov.20170228.093333.815104\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Streaming final output from /tmp/wordcounter2.modintsov.20170228.093333.815104/output...\n",
      "\"\"\t4\n",
      "\"a\"\t3\n",
      "\"ac\"\t4\n",
      "\"adipiscing\"\t1\n",
      "\"aenean\"\t1\n",
      "\"aliquam\"\t2\n",
      "\"aliquam.\"\t1\n",
      "\"aliquet\"\t2\n",
      "\"amet\"\t2\n",
      "\"amet,\"\t1\n",
      "\"at\"\t6\n",
      "\"auctor\"\t4\n",
      "\"auctor,\"\t1\n",
      "\"bibendum\"\t3\n",
      "\"bibendum,\"\t1\n",
      "\"blandit\"\t1\n",
      "\"condimentum\"\t1\n",
      "\"congue\"\t4\n",
      "\"consectetur\"\t3\n",
      "\"consequat\"\t1\n",
      "\"convallis\"\t1\n",
      "\"cras\"\t2\n",
      "\"curabitur\"\t2\n",
      "\"cursus\"\t2\n",
      "\"dapibus\"\t1\n",
      "\"diam\"\t1\n",
      "\"diam.\"\t1\n",
      "\"dictum\"\t1\n",
      "\"dictumst.\"\t1\n",
      "\"dignissim\"\t5\n",
      "\"dolor\"\t2\n",
      "\"dolor.\"\t1\n",
      "\"donec\"\t4\n",
      "\"dui\"\t1\n",
      "\"duis\"\t3\n",
      "\"efficitur\"\t3\n",
      "\"egestas\"\t1\n",
      "\"eget\"\t6\n",
      "\"eget,\"\t2\n",
      "\"eget.\"\t2\n",
      "\"eleifend.\"\t1\n",
      "\"elementum\"\t1\n",
      "\"elementum.\"\t1\n",
      "\"elit\"\t1\n",
      "\"elit,\"\t1\n",
      "\"elit.\"\t1\n",
      "\"enim\"\t2\n",
      "\"enim.\"\t3\n",
      "\"erat\"\t3\n",
      "\"erat.\"\t1\n",
      "\"eros\"\t1\n",
      "\"eros,\"\t1\n",
      "\"est\"\t2\n",
      "\"est,\"\t1\n",
      "\"est.\"\t1\n",
      "\"et\"\t4\n",
      "\"et,\"\t1\n",
      "\"et.\"\t1\n",
      "\"etiam\"\t1\n",
      "\"eu\"\t3\n",
      "\"eu,\"\t1\n",
      "\"euismod\"\t2\n",
      "\"ex\"\t2\n",
      "\"ex.\"\t2\n",
      "\"facilisis\"\t1\n",
      "\"faucibus\"\t3\n",
      "\"felis\"\t2\n",
      "\"felis.\"\t1\n",
      "\"fermentum\"\t2\n",
      "\"finibus\"\t1\n",
      "\"finibus,\"\t1\n",
      "\"fringilla\"\t2\n",
      "\"fusce\"\t2\n",
      "\"gravida\"\t2\n",
      "\"habitasse\"\t1\n",
      "\"hac\"\t1\n",
      "\"hendrerit\"\t2\n",
      "\"iaculis\"\t1\n",
      "\"iaculis.\"\t1\n",
      "\"id\"\t7\n",
      "\"imperdiet\"\t2\n",
      "\"in\"\t6\n",
      "\"in,\"\t1\n",
      "\"integer\"\t2\n",
      "\"interdum.\"\t1\n",
      "\"ipsum\"\t3\n",
      "\"ipsum.\"\t1\n",
      "\"justo\"\t2\n",
      "\"justo.\"\t2\n",
      "\"lacinia\"\t3\n",
      "\"lacus,\"\t1\n",
      "\"laoreet\"\t1\n",
      "\"laoreet.\"\t1\n",
      "\"lectus\"\t3\n",
      "\"lectus,\"\t1\n",
      "\"leo\"\t1\n",
      "\"libero,\"\t1\n",
      "\"libero.\"\t1\n",
      "\"ligula\"\t2\n",
      "\"ligula,\"\t1\n",
      "\"ligula.\"\t1\n",
      "\"lobortis\"\t1\n",
      "\"lobortis.\"\t1\n",
      "\"lorem\"\t1\n",
      "\"luctus\"\t2\n",
      "\"maecenas\"\t2\n",
      "\"magna\"\t4\n",
      "\"massa\"\t5\n",
      "\"massa,\"\t1\n",
      "\"mauris\"\t2\n",
      "\"mauris.\"\t2\n",
      "\"maximus\"\t1\n",
      "\"metus.\"\t1\n",
      "\"mi\"\t1\n",
      "\"molestie\"\t1\n",
      "\"molestie.\"\t1\n",
      "\"mollis,\"\t1\n",
      "\"mollis.\"\t1\n",
      "\"nam\"\t3\n",
      "\"nec\"\t4\n",
      "\"nec,\"\t1\n",
      "\"nec.\"\t1\n",
      "\"neque\"\t1\n",
      "\"neque.\"\t1\n",
      "\"nibh\"\t1\n",
      "\"nibh,\"\t1\n",
      "\"nisi\"\t3\n",
      "\"nisl\"\t2\n",
      "\"nisl,\"\t2\n",
      "\"nisl.\"\t1\n",
      "\"non\"\t6\n",
      "\"nulla\"\t5\n",
      "\"nullam\"\t1\n",
      "\"nunc\"\t3\n",
      "\"nunc,\"\t2\n",
      "\"nunc.\"\t1\n",
      "\"odio,\"\t2\n",
      "\"orci\"\t3\n",
      "\"pellentesque\"\t4\n",
      "\"pharetra\"\t3\n",
      "\"phasellus\"\t3\n",
      "\"placerat\"\t3\n",
      "\"platea\"\t1\n",
      "\"porta\"\t1\n",
      "\"porta,\"\t1\n",
      "\"porttitor\"\t2\n",
      "\"porttitor.\"\t3\n",
      "\"posuere\"\t1\n",
      "\"praesent\"\t2\n",
      "\"proin\"\t1\n",
      "\"pulvinar\"\t2\n",
      "\"purus\"\t1\n",
      "\"purus.\"\t3\n",
      "\"quam\"\t3\n",
      "\"quam.\"\t1\n",
      "\"quis\"\t6\n",
      "\"quis,\"\t1\n",
      "\"quisque\"\t2\n",
      "\"rhoncus\"\t1\n",
      "\"risus\"\t1\n",
      "\"risus,\"\t1\n",
      "\"rutrum\"\t1\n",
      "\"rutrum.\"\t1\n",
      "\"sapien.\"\t1\n",
      "\"scelerisque\"\t1\n",
      "\"scelerisque,\"\t1\n",
      "\"sed\"\t5\n",
      "\"sed,\"\t1\n",
      "\"sem\"\t3\n",
      "\"sem,\"\t2\n",
      "\"sem.\"\t1\n",
      "\"sit\"\t3\n",
      "\"sodales\"\t1\n",
      "\"sodales.\"\t1\n",
      "\"sollicitudin\"\t1\n",
      "\"suspendisse\"\t2\n",
      "\"tempor\"\t4\n",
      "\"tempor.\"\t1\n",
      "\"tempus\"\t3\n",
      "\"tincidunt\"\t5\n",
      "\"tincidunt.\"\t2\n",
      "\"tortor\"\t1\n",
      "\"tortor.\"\t1\n",
      "\"tristique\"\t2\n",
      "\"tristique.\"\t1\n",
      "\"turpis\"\t4\n",
      "\"turpis,\"\t1\n",
      "\"ullamcorper\"\t2\n",
      "\"ultrices\"\t1\n",
      "\"ultrices.\"\t1\n",
      "\"ultricies\"\t1\n",
      "\"urna\"\t2\n",
      "\"ut\"\t3\n",
      "\"ut,\"\t1\n",
      "\"varius\"\t1\n",
      "\"vehicula\"\t1\n",
      "\"vel\"\t6\n",
      "\"vel,\"\t2\n",
      "\"venenatis\"\t1\n",
      "\"vestibulum\"\t5\n",
      "\"vitae\"\t6\n",
      "\"vivamus\"\t1\n",
      "\"viverra\"\t3\n",
      "\"volutpat\"\t3\n",
      "\"volutpat,\"\t2\n",
      "\"volutpat.\"\t1\n",
      "Removing temp directory /tmp/wordcounter2.modintsov.20170228.093333.815104...\n"
     ]
    }
   ],
   "source": [
    "!/home/modintsov/.virtualenvs/ds2017/bin/python wordcounter2.py < loremipsum.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Комбинатор должен иметь такой же формат вывода как маппер, но такую же сигнатуру (входящие параметры) как reducer. Иногда код для reducer совпадает с кодом комбинатора. Часто нет - проверяйте!\n",
    "* Помните: комбинаторы это опциональная оптимизация. Наличие/отсутствие их не должно влиять на корректность алгоритма. Могут быть запущены 0, 1, X раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [In-Mapper Combining](https://dzone.com/articles/designing-mapreduce-algorithms)\n",
    "\n",
    "Комбинатор может быть пропущен, маппер всегда гарантировано запускается.  \n",
    "Так давайте запихнем комбинатор в маппер!  \n",
    "Используем stateful код для этого.  \n",
    "\n",
    "Может запутать все еще сильнее :(\n",
    "\n",
    "Преимущества:\n",
    "\n",
    "* Скорость\n",
    "* Гарантия исполнения\n",
    "\n",
    "Недостатки:\n",
    "\n",
    "* Требуется вручную работать с памятью\n",
    "* Очень легко допустить ошибку\n",
    "* \"Закат солнца вручную\"(с)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcounter3.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcounter3.py\n",
    "from mrjob.job import MRJob\n",
    "from collections import defaultdict\n",
    "\n",
    "class mrWordCount(MRJob):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(mrWordCount, self).__init__(*args, **kwargs)\n",
    "        self.localWordCount = defaultdict(int)\n",
    "    \n",
    "    def mapper(self, key, line):\n",
    "        if False:\n",
    "            yield\n",
    "        for word in line.split(' '):\n",
    "            self.localWordCount[word.lower()] += 1\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        for (word, count) in self.localWordCount.iteritems():\n",
    "            yield word, count\n",
    "\n",
    "    def reducer(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mrWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\r\n",
      "Creating temp directory /tmp/wordcounter3.modintsov.20170228.095435.454158\r\n",
      "Running step 1 of 1...\r\n",
      "reading from STDIN\r\n",
      "Streaming final output from /tmp/wordcounter3.modintsov.20170228.095435.454158/output...\r\n",
      "\"\"\t4\r\n",
      "\"a\"\t3\r\n",
      "\"ac\"\t4\r\n",
      "\"adipiscing\"\t1\r\n",
      "\"aenean\"\t1\r\n",
      "\"aliquam\"\t2\r\n",
      "\"aliquam.\"\t1\r\n",
      "\"aliquet\"\t2\r\n",
      "\"amet\"\t2\r\n",
      "\"amet,\"\t1\r\n",
      "\"at\"\t6\r\n",
      "\"auctor\"\t4\r\n",
      "\"auctor,\"\t1\r\n",
      "\"bibendum\"\t3\r\n",
      "\"bibendum,\"\t1\r\n",
      "\"blandit\"\t1\r\n",
      "\"condimentum\"\t1\r\n",
      "\"congue\"\t4\r\n",
      "\"consectetur\"\t3\r\n",
      "\"consequat\"\t1\r\n",
      "\"convallis\"\t1\r\n",
      "\"cras\"\t2\r\n",
      "\"curabitur\"\t2\r\n",
      "\"cursus\"\t2\r\n",
      "\"dapibus\"\t1\r\n",
      "\"diam\"\t1\r\n",
      "\"diam.\"\t1\r\n",
      "\"dictum\"\t1\r\n",
      "\"dictumst.\"\t1\r\n",
      "\"dignissim\"\t5\r\n",
      "\"dolor\"\t2\r\n",
      "\"dolor.\"\t1\r\n",
      "\"donec\"\t4\r\n",
      "\"dui\"\t1\r\n",
      "\"duis\"\t3\r\n",
      "\"efficitur\"\t3\r\n",
      "\"egestas\"\t1\r\n",
      "\"eget\"\t6\r\n",
      "\"eget,\"\t2\r\n",
      "\"eget.\"\t2\r\n",
      "\"eleifend.\"\t1\r\n",
      "\"elementum\"\t1\r\n",
      "\"elementum.\"\t1\r\n",
      "\"elit\"\t1\r\n",
      "\"elit,\"\t1\r\n",
      "\"elit.\"\t1\r\n",
      "\"enim\"\t2\r\n",
      "\"enim.\"\t3\r\n",
      "\"erat\"\t3\r\n",
      "\"erat.\"\t1\r\n",
      "\"eros\"\t1\r\n",
      "\"eros,\"\t1\r\n",
      "\"est\"\t2\r\n",
      "\"est,\"\t1\r\n",
      "\"est.\"\t1\r\n",
      "\"et\"\t4\r\n",
      "\"et,\"\t1\r\n",
      "\"et.\"\t1\r\n",
      "\"etiam\"\t1\r\n",
      "\"eu\"\t3\r\n",
      "\"eu,\"\t1\r\n",
      "\"euismod\"\t2\r\n",
      "\"ex\"\t2\r\n",
      "\"ex.\"\t2\r\n",
      "\"facilisis\"\t1\r\n",
      "\"faucibus\"\t3\r\n",
      "\"felis\"\t2\r\n",
      "\"felis.\"\t1\r\n",
      "\"fermentum\"\t2\r\n",
      "\"finibus\"\t1\r\n",
      "\"finibus,\"\t1\r\n",
      "\"fringilla\"\t2\r\n",
      "\"fusce\"\t2\r\n",
      "\"gravida\"\t2\r\n",
      "\"habitasse\"\t1\r\n",
      "\"hac\"\t1\r\n",
      "\"hendrerit\"\t2\r\n",
      "\"iaculis\"\t1\r\n",
      "\"iaculis.\"\t1\r\n",
      "\"id\"\t7\r\n",
      "\"imperdiet\"\t2\r\n",
      "\"in\"\t6\r\n",
      "\"in,\"\t1\r\n",
      "\"integer\"\t2\r\n",
      "\"interdum.\"\t1\r\n",
      "\"ipsum\"\t3\r\n",
      "\"ipsum.\"\t1\r\n",
      "\"justo\"\t2\r\n",
      "\"justo.\"\t2\r\n",
      "\"lacinia\"\t3\r\n",
      "\"lacus,\"\t1\r\n",
      "\"laoreet\"\t1\r\n",
      "\"laoreet.\"\t1\r\n",
      "\"lectus\"\t3\r\n",
      "\"lectus,\"\t1\r\n",
      "\"leo\"\t1\r\n",
      "\"libero,\"\t1\r\n",
      "\"libero.\"\t1\r\n",
      "\"ligula\"\t2\r\n",
      "\"ligula,\"\t1\r\n",
      "\"ligula.\"\t1\r\n",
      "\"lobortis\"\t1\r\n",
      "\"lobortis.\"\t1\r\n",
      "\"lorem\"\t1\r\n",
      "\"luctus\"\t2\r\n",
      "\"maecenas\"\t2\r\n",
      "\"magna\"\t4\r\n",
      "\"massa\"\t5\r\n",
      "\"massa,\"\t1\r\n",
      "\"mauris\"\t2\r\n",
      "\"mauris.\"\t2\r\n",
      "\"maximus\"\t1\r\n",
      "\"metus.\"\t1\r\n",
      "\"mi\"\t1\r\n",
      "\"molestie\"\t1\r\n",
      "\"molestie.\"\t1\r\n",
      "\"mollis,\"\t1\r\n",
      "\"mollis.\"\t1\r\n",
      "\"nam\"\t3\r\n",
      "\"nec\"\t4\r\n",
      "\"nec,\"\t1\r\n",
      "\"nec.\"\t1\r\n",
      "\"neque\"\t1\r\n",
      "\"neque.\"\t1\r\n",
      "\"nibh\"\t1\r\n",
      "\"nibh,\"\t1\r\n",
      "\"nisi\"\t3\r\n",
      "\"nisl\"\t2\r\n",
      "\"nisl,\"\t2\r\n",
      "\"nisl.\"\t1\r\n",
      "\"non\"\t6\r\n",
      "\"nulla\"\t5\r\n",
      "\"nullam\"\t1\r\n",
      "\"nunc\"\t3\r\n",
      "\"nunc,\"\t2\r\n",
      "\"nunc.\"\t1\r\n",
      "\"odio,\"\t2\r\n",
      "\"orci\"\t3\r\n",
      "\"pellentesque\"\t4\r\n",
      "\"pharetra\"\t3\r\n",
      "\"phasellus\"\t3\r\n",
      "\"placerat\"\t3\r\n",
      "\"platea\"\t1\r\n",
      "\"porta\"\t1\r\n",
      "\"porta,\"\t1\r\n",
      "\"porttitor\"\t2\r\n",
      "\"porttitor.\"\t3\r\n",
      "\"posuere\"\t1\r\n",
      "\"praesent\"\t2\r\n",
      "\"proin\"\t1\r\n",
      "\"pulvinar\"\t2\r\n",
      "\"purus\"\t1\r\n",
      "\"purus.\"\t3\r\n",
      "\"quam\"\t3\r\n",
      "\"quam.\"\t1\r\n",
      "\"quis\"\t6\r\n",
      "\"quis,\"\t1\r\n",
      "\"quisque\"\t2\r\n",
      "\"rhoncus\"\t1\r\n",
      "\"risus\"\t1\r\n",
      "\"risus,\"\t1\r\n",
      "\"rutrum\"\t1\r\n",
      "\"rutrum.\"\t1\r\n",
      "\"sapien.\"\t1\r\n",
      "\"scelerisque\"\t1\r\n",
      "\"scelerisque,\"\t1\r\n",
      "\"sed\"\t5\r\n",
      "\"sed,\"\t1\r\n",
      "\"sem\"\t3\r\n",
      "\"sem,\"\t2\r\n",
      "\"sem.\"\t1\r\n",
      "\"sit\"\t3\r\n",
      "\"sodales\"\t1\r\n",
      "\"sodales.\"\t1\r\n",
      "\"sollicitudin\"\t1\r\n",
      "\"suspendisse\"\t2\r\n",
      "\"tempor\"\t4\r\n",
      "\"tempor.\"\t1\r\n",
      "\"tempus\"\t3\r\n",
      "\"tincidunt\"\t5\r\n",
      "\"tincidunt.\"\t2\r\n",
      "\"tortor\"\t1\r\n",
      "\"tortor.\"\t1\r\n",
      "\"tristique\"\t2\r\n",
      "\"tristique.\"\t1\r\n",
      "\"turpis\"\t4\r\n",
      "\"turpis,\"\t1\r\n",
      "\"ullamcorper\"\t2\r\n",
      "\"ultrices\"\t1\r\n",
      "\"ultrices.\"\t1\r\n",
      "\"ultricies\"\t1\r\n",
      "\"urna\"\t2\r\n",
      "\"ut\"\t3\r\n",
      "\"ut,\"\t1\r\n",
      "\"varius\"\t1\r\n",
      "\"vehicula\"\t1\r\n",
      "\"vel\"\t6\r\n",
      "\"vel,\"\t2\r\n",
      "\"venenatis\"\t1\r\n",
      "\"vestibulum\"\t5\r\n",
      "\"vitae\"\t6\r\n",
      "\"vivamus\"\t1\r\n",
      "\"viverra\"\t3\r\n",
      "\"volutpat\"\t3\r\n",
      "\"volutpat,\"\t2\r\n",
      "\"volutpat.\"\t1\r\n",
      "Removing temp directory /tmp/wordcounter3.modintsov.20170228.095435.454158...\r\n"
     ]
    }
   ],
   "source": [
    "!/home/modintsov/.virtualenvs/ds2017/bin/python wordcounter3.py < loremipsum.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что лучше выбрать?\n",
    "\n",
    "* Для больших словарей?\n",
    "* Для словарей с неравномерным распределением?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие алгоритмы ML воспроизведены на архитектуре MapReduce.\n",
    "* [SVM](http://www.sonaliagarwal.com/anu.pdf)\n",
    "* [Random Forest](http://www.vision.cs.chubu.ac.jp/MPRG/C_group/C075_wakayama2015.pdf)  \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Spark](https://en.wikipedia.org/wiki/Apache_Spark)\n",
    "\n",
    "Зачем создавать сабсеты шаг за шагом по мере надобности? Мы можем создать их все, записать на диск и отдать параллельным процессам на обработку.  \n",
    "Если мы работаем с одним куском данных - зачем копировать на диск - это медленно. Копируем кусок памяти и отдадим его.  \n",
    "Если мы никогда ничего не пишем в данные - зачем копровать ВООБЩЕ? Отдадим многим процессам один общий кусок данных для чтения.  \n",
    "\n",
    "Некоторые элементы Data Science [\"Embarrasingly parallel\"](https://en.wikipedia.org/wiki/Embarrassingly_parallel).\n",
    "\n",
    "1. Cross Validation\n",
    "2. Grid Search\n",
    "3. Bagging/Random Forests\n",
    "...\n",
    "\n",
    "Если вы работаете только на одной машине - [multiprocessing](https://docs.python.org/2/library/multiprocessing.html) ваш друг.\n",
    "\n",
    "[Мануал](https://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/) по работе с multiprocessing.\n",
    "\n",
    "1. Мастер-процесс подготавливает данные\n",
    "2. Постулируем, что эти данные будут исключительно read-only\n",
    "3. Создадим пул рабочих подпроцессов, а в каждом из них - пул подпроцессов для grid search.\n",
    "4. Каждый рабочий роцесс вычисляет validation score\n",
    "5. Один \"reducer\" вычисляет средние значения по gridsearch процессам, другой - выбирает максимум из средних.\n",
    "6. Отдаем параметры \"наверх\" в мастер процесс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop, как и многие реализации MapReduce сохраняет результат каждой промежуточной операции на диск. Следующие шаги загружают с диска.\n",
    "Почему? Операции с диском затратные!\n",
    "\n",
    "\n",
    "Checkpoints! Шаг закончился успешно - сохрани. Не закончился - вернись к предыдущему этапу и повтори на новом процессе.  \n",
    "Ошибки параллельного программирования - ад дебага.  \n",
    "Машины в облаке умирают.  \n",
    "Не всегда фиксить ошибки - лучшее решение. Даже гугл так делает.\n",
    "\n",
    "Разработчики sklearn тоже это знают и понимают. [joblib](https://pypi.python.org/pypi/joblib) используется во многих внутренних процессах при n_jobs>1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что если ваши данные БОЛЬШИЕ?   \n",
    "Запихнем их в кластер и используем MapReduce.  \n",
    "Коммуникации медленные, работа с диском медленная :(\n",
    "\n",
    "MapReduce - идея [функционального программирования](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5_%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5).  \n",
    "Никаких [побочных эффектов](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B1%D0%BE%D1%87%D0%BD%D1%8B%D0%B9_%D1%8D%D1%84%D1%84%D0%B5%D0%BA%D1%82_(%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5)), никакого состояния.  \n",
    "Функции соединены потоком данных.  \n",
    "Гарантия \"восстанавливаемого\" состояния в любой момент.\n",
    "\n",
    "![](https://www.mapr.com/sites/default/files/blogimages/Spark-core-stack-DB.jpg)\n",
    "\n",
    "![](http://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
    "\n",
    "![](https://image.slidesharecdn.com/numbaspark-160406151628/95/gpu-computing-with-apache-spark-and-python-30-638.jpg?cb=1459955819)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с Hadoop (почему Spark лучше)\n",
    "\n",
    "1. Может работать в памяти\n",
    "2. Данные кешируются (диск/память) для дальнейшего исполбзования\n",
    "3. **Скорость** - сортировка 100TB занимает 23 минуты на 206 машинах в Spark, 72 минуты на 2100 машинах в Hadoop (эксперимент 2013 года).\n",
    "4. [RDD](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm) (resilient distrivuted dataset) - абстракция поверх источника данных, оптимизированная для параллельных вычислений. API позволяет легко запрашивать данные с каждого этапа конвеера вычислений при этом избегая избыточных вычислений и затрат на репликацию.\n",
    "5. Python/Java/Scala\n",
    "6. Легче чем Hadoop во многих аспектах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark работает исполняя программу-драйвер, которая выполняет параллельные операции на RDD в кластере (или на локальной машине).  \n",
    "Типы операций :\n",
    "1. Transformations - создают новый RDD из существующего (помним, read-only!). Аналог map.\n",
    "2. Actions - возвращают данные в драйвер. Аналог reduce.\n",
    "\n",
    "[Короткие примеры](http://spark.apache.org/examples.html)\n",
    "\n",
    "```\n",
    "text_file = sc.textFile(\"hdfs://...\")\n",
    "# OR sc.parallelize(['list', 'of', 'values'])\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs://...\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительный функционал\n",
    "\n",
    "1. Прямая работа с dataframes, файлами, базами данных...\n",
    "2. Работа с RDD через SQL\n",
    "3. Потоковая обработка и онлайн-алгоритмы\n",
    "4. Библиотека MLLIB\n",
    "5. Операции на графах (GraphX)\n",
    "6. Коннекторы к другим облачным приложениям (YARN, MESOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Инструкция по установке на Linux](https://www.dataquest.io/blog/pyspark-installation-guide/)\n",
    "\n",
    "У меня не завелось в юпитере :( скорее всего конфликт версий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все в консоль! {spark_install_dir}/bin/pyspark\n",
    "\n",
    "Примеры кода для тестов \n",
    "\n",
    "```\n",
    "sc\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).map(lambda x: x**2).sum()\n",
    "\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "print type(wordsRDD)\n",
    "\n",
    "# Laaaaazy...until \n",
    "wordsRDD.collect()\n",
    "\n",
    "№ Cache on!\n",
    "wordsRDD.cache()\n",
    "\n",
    "def makePlural(word):\n",
    "    return word + 's'\n",
    "print makePlural('cat')\n",
    "\n",
    "pluralRDD = wordsRDD.map(makePlural)\n",
    "print pluralRDD.first()\n",
    "print pluralRDD.take(2)\n",
    "\n",
    "pluralRDD.collect()\n",
    "\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda w: (w, 1))\n",
    "                       .reduceByKey(lambda x,y: x+y)\n",
    "                       .collect())\n",
    "print wordCountsCollected\n",
    "\n",
    "print (wordsRDD\n",
    "    .map(lambda w: (w, 1))\n",
    "    .reduceByKey(lambda x,y: x+y)).toDebugString()\n",
    "    \n",
    "stopwords=[e.strip() for e in open(\"/home/modintsov/workspace/datascienceua_2017/lesson_12/english.stop.txt\").readlines()]\n",
    "\n",
    "juliusrdd=sc.textFile(\"/home/modintsov/workspace/datascienceua_2017/lesson_12/caesar.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "juliusrdd.flatMap(lambda line: line.split()).count()\n",
    "\n",
    "juliusrdd.flatMap(lambda line: line.split()).map(lambda word: word.strip().lower()).take(20)\n",
    "\n",
    "juliusrdd.flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: word.strip().lower())\\\n",
    "    .filter(lambda word: word not in stopwords).take(20)\n",
    "    \n",
    "juliusrdd.flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: word.strip().lower())\\\n",
    "    .filter(lambda word: word not in stopwords)\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b).take(20)\n",
    "    \n",
    "juliusrdd.flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: word.strip().lower())\\\n",
    "    .filter(lambda word: word not in stopwords)\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .takeOrdered(20, lambda x: -x[1])\n",
    "    \n",
    "captions, counts=zip(*juliusrdd.flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: word.strip().lower())\\\n",
    "    .filter(lambda word: word not in stopwords)\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .takeOrdered(20, lambda x: -x[1]))\n",
    "\n",
    "pos = np.arange(len(counts))\n",
    "plt.bar(pos, counts)\n",
    "plt.xticks(pos+0.4, captions, rotation=90)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
